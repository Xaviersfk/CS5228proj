{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIzh+2HBADOvXE1+h/LwYA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4WH9k-hHjnB9"},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","from nltk.chunk import ne_chunk\n","from collections import Counter, defaultdict\n","import networkx as nx\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('conll2002')\n","\n","\n","# Load the dataset\n","dataset_path = 'fulltrain.csv'\n","df = pd.read_csv(dataset_path)\n","\n","# Basic Dataset Information\n","num_rows, num_cols = df.shape\n","print(f\"Number of rows: {num_rows}\")\n","print(f\"Number of columns: {num_cols}\")\n","print(df.head())\n","missing_values = df.isnull().sum()\n","print(\"Missing values:\")\n","print(missing_values)\n","\n","# Dataset statistics\n","print(df.info())\n","\n","# Analyzing text data\n","df['Word Count'] = df.iloc[:, 1].apply(lambda x: len(str(x).split()))\n","print(df['Word Count'].describe())\n","\n","max_row = df.loc[df['Word Count'].idxmax()]\n","print(\"Row with maximum words:\")\n","print(max_row)\n","\n","min_row = df.loc[df['Word Count'].idxmin()]\n","print(\"Row with minimum words:\")\n","print(min_row)\n","\n","word_counts = Counter(\" \".join(df.iloc[:, 1]).split())\n","most_common_word = word_counts.most_common(1)[0]\n","print(\"Most common word:\", most_common_word[0], \"appears:\", most_common_word[1], \"times\")\n","\n","# Named Entity Recognition\n","with open(dataset_path, 'r', encoding='utf-8') as file:\n","    data = file.read()\n","\n","tokens = word_tokenize(data[:1000000])  # Adjust according to file size\n","pos_tags = pos_tag(tokens)\n","ner_tags = ne_chunk(pos_tags)\n","\n","# Extract entities\n","persons, locations, organizations = [], [], []\n","for subtree in ner_tags:\n","    if isinstance(subtree, nltk.Tree):\n","        entity_type = subtree.label()\n","        entity_tokens = [token for token, tag in subtree.leaves()]\n","        if entity_type == 'PERSON':\n","            persons.append(' '.join(entity_tokens))\n","        elif entity_type == 'GPE':\n","            locations.append(' '.join(entity_tokens))\n","        elif entity_type == 'ORGANIZATION':\n","            organizations.append(' '.join(entity_tokens))\n","\n","print(\"Persons:\", persons)\n","print(\"Locations:\", locations)\n","print(\"Organizations:\", organizations)\n","\n","# Visualizations\n","dataset.hist(figsize=(10, 8))\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","sns.boxplot(data=dataset)\n","plt.show()\n","\n","sns.scatterplot(x='feature1', y='feature2', data=dataset)\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Relationship between Feature 1 and Feature 2')\n","plt.show()\n","\n","# Build and visualize an entity graph\n","G = nx.Graph()\n","for entity_list in [persons, locations, organizations]:\n","    for entity in entity_list:\n","        G.add_node(entity, type=entity_list[0])  # Assuming first element represents entity type\n","        for other_entity in entity_list:\n","            if entity != other_entity:\n","                G.add_edge(entity, other_entity)\n","\n","pos = nx.spring_layout(G, seed=42)\n","plt.figure(figsize=(12, 8))\n","nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=10, font_weight='bold')\n","plt.title('Named Entity Relations')\n","plt.show()\n"]}]}